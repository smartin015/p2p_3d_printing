{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"PeerPrint PeerPrint decentralizes 3D printing - any 3D printer connects to any print queue on any network. Features Zero-config discovery - print services (such as OctoPrint) can self-organize and find each other over the network (both LAN and WAN) using libp2p libraries. Print job replication - print jobs are tracked in queues, with their data stored reliably across multiple peers using the RAFT consensus algorithm for replication. Collaborate without limits - whether it's a couple printers in your garage or millions of printers across the world, PeerPrint is designed to scale up to the challenge. Why use PeerPrint? Decentralized 3D printing is a solution whenever 3d printer reliability and bandwidth are a problem. Here are a few examples: Disaster mitigation - join a global queue to contribute masks, ventilator parts, and other scarce supplies on-demand for the next pandemic. Hobbyist communities - issue print orders to mass-produce blasters for a NERF battle, or airframes for a quadcopter competition. Product Fulfillment - distribute printed objects directly to a customer base, without having to ship physical goods. Complex Assemblies - print thousands of distinct parts quickly and combine them into a finished product that would otherwise take thousands of hours to make. How does it work? PeerPrint is run alongside existing printer software which already controls print execution and treats PeerPrint as a source of work to be done. It connects to other PeerPrint servers to synchronize the state of the queue among them. See Network Architecture for details on how peers connect to each other, and Process Architecture for how the server runs and communicates to a host process. How do I get involved? Currently, PeerPrint is being integrated into the Continuous Print Queue plugin for OctoPrint. See CPQ documentation for instructions on how to get started running network queues. If you want to host your own queue, see Creating Queues . If you want to develop on PeerPrint, see Contributing for how to set up an environment.","title":"Home"},{"location":"#peerprint","text":"PeerPrint decentralizes 3D printing - any 3D printer connects to any print queue on any network.","title":"PeerPrint"},{"location":"#features","text":"Zero-config discovery - print services (such as OctoPrint) can self-organize and find each other over the network (both LAN and WAN) using libp2p libraries. Print job replication - print jobs are tracked in queues, with their data stored reliably across multiple peers using the RAFT consensus algorithm for replication. Collaborate without limits - whether it's a couple printers in your garage or millions of printers across the world, PeerPrint is designed to scale up to the challenge.","title":"Features"},{"location":"#why-use-peerprint","text":"Decentralized 3D printing is a solution whenever 3d printer reliability and bandwidth are a problem. Here are a few examples: Disaster mitigation - join a global queue to contribute masks, ventilator parts, and other scarce supplies on-demand for the next pandemic. Hobbyist communities - issue print orders to mass-produce blasters for a NERF battle, or airframes for a quadcopter competition. Product Fulfillment - distribute printed objects directly to a customer base, without having to ship physical goods. Complex Assemblies - print thousands of distinct parts quickly and combine them into a finished product that would otherwise take thousands of hours to make.","title":"Why use PeerPrint?"},{"location":"#how-does-it-work","text":"PeerPrint is run alongside existing printer software which already controls print execution and treats PeerPrint as a source of work to be done. It connects to other PeerPrint servers to synchronize the state of the queue among them. See Network Architecture for details on how peers connect to each other, and Process Architecture for how the server runs and communicates to a host process.","title":"How does it work?"},{"location":"#how-do-i-get-involved","text":"Currently, PeerPrint is being integrated into the Continuous Print Queue plugin for OctoPrint. See CPQ documentation for instructions on how to get started running network queues. If you want to host your own queue, see Creating Queues . If you want to develop on PeerPrint, see Contributing for how to set up an environment.","title":"How do I get involved?"},{"location":"contributing/","text":"Contributing Warning These docs are still under construction Setting up a dev environment git clone https://github.com/smartin015/peerprint.git Build the docker image and launch it, then set up plugin dev docker-compose build dev docker-compose run dev /bin/bash pip3 install -e . Build all the files: protoc --go_out=. --python-out=. --go_opt=paths=source_relative proto/*.proto go build . Run the wan queue demo to ensure everything is set up well export PATH=$PATH:$(pwd)/server python3 -m peerprint.wan.demo Demo using IPFS registry In a separate console, run ipfs init ipfs add example_registry.yaml ipfs daemon Make a note of the CID of the published registry, then launch the service: ./peerprint_server -registry $IPFS_REGISTRY_CID","title":"Contributing"},{"location":"contributing/#contributing","text":"Warning These docs are still under construction","title":"Contributing"},{"location":"contributing/#setting-up-a-dev-environment","text":"git clone https://github.com/smartin015/peerprint.git Build the docker image and launch it, then set up plugin dev docker-compose build dev docker-compose run dev /bin/bash pip3 install -e . Build all the files: protoc --go_out=. --python-out=. --go_opt=paths=source_relative proto/*.proto go build . Run the wan queue demo to ensure everything is set up well export PATH=$PATH:$(pwd)/server python3 -m peerprint.wan.demo","title":"Setting up a dev environment"},{"location":"contributing/#demo-using-ipfs-registry","text":"In a separate console, run ipfs init ipfs add example_registry.yaml ipfs daemon Make a note of the CID of the published registry, then launch the service: ./peerprint_server -registry $IPFS_REGISTRY_CID","title":"Demo using IPFS registry"},{"location":"network-architecture/","text":"Network Architecture Overview The Registry hosts metadata describing available queues to join. This is a yaml file typically hosted on IPFS that includes the set of trusted peers and the rendezvous string, among other queue details. Trusted Peers are responsible for maintaining the state of the queue and assigning roles to untrusted peers. These peers can also do work on the queue. Other Peers interact with trusted peers to do work on the queue. Communication All trusted peers are connected to each other ( N^2 connections) and form a RAFT consensus group. This is the system which stores and synchronizes queue data across all peers. It is recommended to have at least 3 trusted peers, as that provides consensus guarantees and prevents data loss when one or more peers go offline. Additional trusted peers increases reliability, but there are diminishing returns as more network connections and cross-talk is needed to maintain the RAFT log. All other peers are connected over libp2p pubsub - in this way, network connections are minimized but all peers are still indirectly connected. Scaling Unlimited peering over pubsub presents challenges to scalability. Understanding the network While local networks can almost always query the whole population of connected peers for their state, this becomes infeasible as the peer count scales into the millions. For this reason, peers are asked for their state probabilistically - each peer responds with its state with a probability p < 1.0 . The value of p is currently constant, but in the future the leader will use previous poll results to dynamically adjust p to obtain a well-bounded sample of peers (say, ~50). Previous poll results can also be aggregated to increase certainty. Tallying up the number of peers is then turned into a binomial distribution problem, from which we can estimate the total population of peers and their expected states. Managing queue size Millions of peers would require millions of jobs to keep them busy. But host RAM is limited, so a peer's view into a queue must be bounded. For this reason, a \"queue\" is actually a collection of smaller instances of the original network architecture that hold the same purpose. As the queue grows, the leader eventually must decide to split off work from the queue into multiple 'shards' (represented as pubsub topics). Each shard has its own RAFT log and consensus group that manages the state of the queue. Note that this means RAFT consensus groups scale linearly with the number of jobs, and so job count is actually bounded by the number of peers in the queue. Future work may be needed to identify job saturation and prevent further jobs from being added until there is sufficient peer capacity to host them. Because consensus groups may need to be \"minted\" as the queue expands, the initial set of trusted peers must also have some way to delegate authority of these shards. This has yet to be implemented.","title":"Network Architecture"},{"location":"network-architecture/#network-architecture","text":"","title":"Network Architecture"},{"location":"network-architecture/#overview","text":"The Registry hosts metadata describing available queues to join. This is a yaml file typically hosted on IPFS that includes the set of trusted peers and the rendezvous string, among other queue details. Trusted Peers are responsible for maintaining the state of the queue and assigning roles to untrusted peers. These peers can also do work on the queue. Other Peers interact with trusted peers to do work on the queue.","title":"Overview"},{"location":"network-architecture/#communication","text":"All trusted peers are connected to each other ( N^2 connections) and form a RAFT consensus group. This is the system which stores and synchronizes queue data across all peers. It is recommended to have at least 3 trusted peers, as that provides consensus guarantees and prevents data loss when one or more peers go offline. Additional trusted peers increases reliability, but there are diminishing returns as more network connections and cross-talk is needed to maintain the RAFT log. All other peers are connected over libp2p pubsub - in this way, network connections are minimized but all peers are still indirectly connected.","title":"Communication"},{"location":"network-architecture/#scaling","text":"Unlimited peering over pubsub presents challenges to scalability.","title":"Scaling"},{"location":"network-architecture/#understanding-the-network","text":"While local networks can almost always query the whole population of connected peers for their state, this becomes infeasible as the peer count scales into the millions. For this reason, peers are asked for their state probabilistically - each peer responds with its state with a probability p < 1.0 . The value of p is currently constant, but in the future the leader will use previous poll results to dynamically adjust p to obtain a well-bounded sample of peers (say, ~50). Previous poll results can also be aggregated to increase certainty. Tallying up the number of peers is then turned into a binomial distribution problem, from which we can estimate the total population of peers and their expected states.","title":"Understanding the network"},{"location":"network-architecture/#managing-queue-size","text":"Millions of peers would require millions of jobs to keep them busy. But host RAM is limited, so a peer's view into a queue must be bounded. For this reason, a \"queue\" is actually a collection of smaller instances of the original network architecture that hold the same purpose. As the queue grows, the leader eventually must decide to split off work from the queue into multiple 'shards' (represented as pubsub topics). Each shard has its own RAFT log and consensus group that manages the state of the queue. Note that this means RAFT consensus groups scale linearly with the number of jobs, and so job count is actually bounded by the number of peers in the queue. Future work may be needed to identify job saturation and prevent further jobs from being added until there is sufficient peer capacity to host them. Because consensus groups may need to be \"minted\" as the queue expands, the initial set of trusted peers must also have some way to delegate authority of these shards. This has yet to be implemented.","title":"Managing queue size"},{"location":"new-queue/","text":"Creating Queues Overview PeerPrint is decentralized - you can create your own global network queue that you can use for whatever purpose you like, with no restrictions. Requirements Creating a new queue requires: The public keys (IDs) of 3 or more peers to consider as trusted peers (see Gathering IDs ) A working IPFS installation for publishing the registry. A way to pass the registry's location along to everyone you want to join the queue. This could be as simple as pencil-and-paper or as buttery-smooth as a DNSLink record Gathering IDs You can gather the IDs of your trusted peers by running peerprint with -privkeyfile and -pubkeyfile set to a known location, then reading the logs to get the ID string of the peer. Note that you must provide these same paths when you next run the process for it to load this same identity. DNSLink domain names For convenience of other users, you can use DNSLink to link your registry's CID and make it human-discoverable. This allows other users to join your registry by visiting a \"standard\" domain name (e.g. continuousprint.net), but it does introduce the potential for failure / censorship if for whatever reason your DNS provider stops hosting your DNSLink record. More decentralized domain ownership could be done via handshake as that effort develops. Create/Update the Registry Once you've gathered your trusted peers and have your IPFS daemon up and running, it's time to host the queue. First, we need to generate a rendezvous string that all peers will use to discover each other - you can do this on Ubuntu with dbus-uuidgen , or looking up the current unix timestamp, or using an online UUID tool - anything that provides a unique string that's unlikely to collide with other network queues. Next, put all your information into a registry.yaml file in the following format: created: <put the current unix timestamp here> url: <link to more information about the registry maintainer> queues: - name: <a brief string to distinguish this queue from others in the registry, e.g. \"testqueue\"> desc: <a simple queue description> url: <link to more information about the queue> rendezvous: <your rendezvous string here> trustedPeers: - <trusted peers go here on individual lines, something like \"12D3KooWNgjdBgmgRyY42Eo2Jg3rGyvaepU4QDkEMjy4WtF3Ad9V\"> Now open a console and run ipfs add registry.yaml . Make a note of the CID under which it was published. If you are using DNSLink, add the CID to your domain as a TXT record, e.g: TXT _dnslink.registry.continuousprint.net dnslink=/ipns/QmYwAPJzv5CZsnA625s3Xf2nemtYgPpHdWEz79ojWnPbdG Bootstrapping with trusted peers Now that your registry is available, pass your CID / DNSLink address as -registry in your execution of peerprint_server before starting your trusted peers. Make sure to also point -pubkeyfile and -privkeyfile to where your peers' ID was generated, or else they won't be considered trusted). The peers should discover one another, elect a leader amongst themselves, take a poll of how many peers are connected to the queue, and then wait for further instruction. Congrats on setting up a fully distributed network queue with PeerPrint!","title":"Creating Queues"},{"location":"new-queue/#creating-queues","text":"","title":"Creating Queues"},{"location":"new-queue/#overview","text":"PeerPrint is decentralized - you can create your own global network queue that you can use for whatever purpose you like, with no restrictions.","title":"Overview"},{"location":"new-queue/#requirements","text":"Creating a new queue requires: The public keys (IDs) of 3 or more peers to consider as trusted peers (see Gathering IDs ) A working IPFS installation for publishing the registry. A way to pass the registry's location along to everyone you want to join the queue. This could be as simple as pencil-and-paper or as buttery-smooth as a DNSLink record","title":"Requirements"},{"location":"new-queue/#gathering-ids","text":"You can gather the IDs of your trusted peers by running peerprint with -privkeyfile and -pubkeyfile set to a known location, then reading the logs to get the ID string of the peer. Note that you must provide these same paths when you next run the process for it to load this same identity.","title":"Gathering IDs"},{"location":"new-queue/#dnslink-domain-names","text":"For convenience of other users, you can use DNSLink to link your registry's CID and make it human-discoverable. This allows other users to join your registry by visiting a \"standard\" domain name (e.g. continuousprint.net), but it does introduce the potential for failure / censorship if for whatever reason your DNS provider stops hosting your DNSLink record. More decentralized domain ownership could be done via handshake as that effort develops.","title":"DNSLink domain names"},{"location":"new-queue/#createupdate-the-registry","text":"Once you've gathered your trusted peers and have your IPFS daemon up and running, it's time to host the queue. First, we need to generate a rendezvous string that all peers will use to discover each other - you can do this on Ubuntu with dbus-uuidgen , or looking up the current unix timestamp, or using an online UUID tool - anything that provides a unique string that's unlikely to collide with other network queues. Next, put all your information into a registry.yaml file in the following format: created: <put the current unix timestamp here> url: <link to more information about the registry maintainer> queues: - name: <a brief string to distinguish this queue from others in the registry, e.g. \"testqueue\"> desc: <a simple queue description> url: <link to more information about the queue> rendezvous: <your rendezvous string here> trustedPeers: - <trusted peers go here on individual lines, something like \"12D3KooWNgjdBgmgRyY42Eo2Jg3rGyvaepU4QDkEMjy4WtF3Ad9V\"> Now open a console and run ipfs add registry.yaml . Make a note of the CID under which it was published. If you are using DNSLink, add the CID to your domain as a TXT record, e.g: TXT _dnslink.registry.continuousprint.net dnslink=/ipns/QmYwAPJzv5CZsnA625s3Xf2nemtYgPpHdWEz79ojWnPbdG","title":"Create/Update the Registry"},{"location":"new-queue/#bootstrapping-with-trusted-peers","text":"Now that your registry is available, pass your CID / DNSLink address as -registry in your execution of peerprint_server before starting your trusted peers. Make sure to also point -pubkeyfile and -privkeyfile to where your peers' ID was generated, or else they won't be considered trusted). The peers should discover one another, elect a leader amongst themselves, take a poll of how many peers are connected to the queue, and then wait for further instruction. Congrats on setting up a fully distributed network queue with PeerPrint!","title":"Bootstrapping with trusted peers"},{"location":"process-architecture/","text":"Process Architecture Server and Wrapper PeerPrint is broken down into two parts: the server and the wrapper. The server manages connections among 3D print servers and synchronizes their state. The wrapper allows other software (e.g. OctoPrint) to interact with the server. The Continuous Print Queue plugin is the first such software to use the wrapper. Local Communication The wrapper translates queue commands into protocol buffers , which are then serialized and sent to the server (by default, using ZeroMQ UNIX sockets ) to be executed. This is done with a REP/REQ socket pair, with the server returning either an OK message or an error with a status string. The wrapper also receives asynchronous updates from the server using a PUSH/PULL socket pair, deserializing the received protobufs and triggering callbacks into the host process. Message data is additionally cached by the wrapper so that looking up print jobs, peers etc. requires zero additional communications between server and wrapper. The server's log messages are also sent over a zmq PUSH socket and received by the wrapper for insertion into the host process' logs. Life cycle Peerprint's life begins when the host process (e.g. the Continuous Print Queue plugin from OctoPrint) calls the connect function of the wrapper class. This forks off a new suprocess to manage the p2p connection and queue state, which initializes by: Fetching configuration (the \"registry\") and identity (public & private keys) Discovering peers based on a rendezvous string (either MDNS for LAN queues or via DHT for WAN queues) Establishing leadership (via RAFT consensus if a participant in elections, otherwise by subscribing to the current elected leader) Recovering state (from RAFT logs) Among other arguments, the name of the queue to join is passed via -queue command line argument. In addition to forking off the process, the wrapper also initializes ZMQ sockets to interact with the server's command, update, and log streams. Registry acquisition The server first attempts to load a registry - this can either be a local file or a CID reference to a file hosted on IPFS. The regstry contains a list of queues and metadata on how to connect and interact with them. The target queue data is parsed and used later in the Discovery step. Key / Identity generation Identity of peers is important, as this dictates who is trusted as part of the RAFT leadership pool, who holds leases on shared state objects, and may eventually be part of role-based access control. When PeerPrint is run for the first time, a public/private key pair is generated and saved (location specified by -privkeyfile and -pubkeyfile ). This key pair is reused on consecutive runs and establishes the identity of the printer/server using PeerPrint. Discovery After the prerequisite steps above, PeerPrint spends a few seconds searching for peers of the queue it wishes to join. This is done via distributed hash table rendevous (DHT rendezvous implementation summary here ). In either case, the name of the queue (from -queue ) is used as the rendezvous string. If -local is provided via argv, then MDNS is used instead to look for peers on the local network. In this case, non-local peers cannot be discovered and the queue is LAN-only. Leader election and synchronization After the discovery period is over and peers have been found, PeerPrint falls into one of two roles: If the peer's public key is not listed under trustedPeers in the manifest for the joined queue, it is a LISTENER (excluded from participation in RAFT) and it asks the current leader for an assignment to a pubsub topic. If the peer is in trustedPeers , it is considered ELECTABLE and it asks for - and connects to - the other electable peers, before starting its own RAFT server and joining in elections. The state of the queue is eventually passed to the server either via the leader (if a listener) or via looking up the state of the raft queue (if electable). This is then forwarded on to the wrapper and the host process. Job operations When a wrapper receives a request to mutate a job, it sends this request over ZMQ socket to the server. The server then communicates the request remotely to the leader via lib2p2 pubsub . Use of pubsub for command execution allows for a nearly unbounded number of peers to contribute to a network queue. The leader receives this message and validates it before committing any mutations to the RAFT log. The log state is then synchronized to other electable peers via RAFT log replication, and published to all remaining peers over pubsub. As peers receive the update, they push it to the wrapper (again over ZMQ socket) which notifies the host process.","title":"Process Architecture"},{"location":"process-architecture/#process-architecture","text":"","title":"Process Architecture"},{"location":"process-architecture/#server-and-wrapper","text":"PeerPrint is broken down into two parts: the server and the wrapper. The server manages connections among 3D print servers and synchronizes their state. The wrapper allows other software (e.g. OctoPrint) to interact with the server. The Continuous Print Queue plugin is the first such software to use the wrapper.","title":"Server and Wrapper"},{"location":"process-architecture/#local-communication","text":"The wrapper translates queue commands into protocol buffers , which are then serialized and sent to the server (by default, using ZeroMQ UNIX sockets ) to be executed. This is done with a REP/REQ socket pair, with the server returning either an OK message or an error with a status string. The wrapper also receives asynchronous updates from the server using a PUSH/PULL socket pair, deserializing the received protobufs and triggering callbacks into the host process. Message data is additionally cached by the wrapper so that looking up print jobs, peers etc. requires zero additional communications between server and wrapper. The server's log messages are also sent over a zmq PUSH socket and received by the wrapper for insertion into the host process' logs.","title":"Local Communication"},{"location":"process-architecture/#life-cycle","text":"Peerprint's life begins when the host process (e.g. the Continuous Print Queue plugin from OctoPrint) calls the connect function of the wrapper class. This forks off a new suprocess to manage the p2p connection and queue state, which initializes by: Fetching configuration (the \"registry\") and identity (public & private keys) Discovering peers based on a rendezvous string (either MDNS for LAN queues or via DHT for WAN queues) Establishing leadership (via RAFT consensus if a participant in elections, otherwise by subscribing to the current elected leader) Recovering state (from RAFT logs) Among other arguments, the name of the queue to join is passed via -queue command line argument. In addition to forking off the process, the wrapper also initializes ZMQ sockets to interact with the server's command, update, and log streams.","title":"Life cycle"},{"location":"process-architecture/#registry-acquisition","text":"The server first attempts to load a registry - this can either be a local file or a CID reference to a file hosted on IPFS. The regstry contains a list of queues and metadata on how to connect and interact with them. The target queue data is parsed and used later in the Discovery step.","title":"Registry acquisition"},{"location":"process-architecture/#key-identity-generation","text":"Identity of peers is important, as this dictates who is trusted as part of the RAFT leadership pool, who holds leases on shared state objects, and may eventually be part of role-based access control. When PeerPrint is run for the first time, a public/private key pair is generated and saved (location specified by -privkeyfile and -pubkeyfile ). This key pair is reused on consecutive runs and establishes the identity of the printer/server using PeerPrint.","title":"Key / Identity generation"},{"location":"process-architecture/#discovery","text":"After the prerequisite steps above, PeerPrint spends a few seconds searching for peers of the queue it wishes to join. This is done via distributed hash table rendevous (DHT rendezvous implementation summary here ). In either case, the name of the queue (from -queue ) is used as the rendezvous string. If -local is provided via argv, then MDNS is used instead to look for peers on the local network. In this case, non-local peers cannot be discovered and the queue is LAN-only.","title":"Discovery"},{"location":"process-architecture/#leader-election-and-synchronization","text":"After the discovery period is over and peers have been found, PeerPrint falls into one of two roles: If the peer's public key is not listed under trustedPeers in the manifest for the joined queue, it is a LISTENER (excluded from participation in RAFT) and it asks the current leader for an assignment to a pubsub topic. If the peer is in trustedPeers , it is considered ELECTABLE and it asks for - and connects to - the other electable peers, before starting its own RAFT server and joining in elections. The state of the queue is eventually passed to the server either via the leader (if a listener) or via looking up the state of the raft queue (if electable). This is then forwarded on to the wrapper and the host process.","title":"Leader election and synchronization"},{"location":"process-architecture/#job-operations","text":"When a wrapper receives a request to mutate a job, it sends this request over ZMQ socket to the server. The server then communicates the request remotely to the leader via lib2p2 pubsub . Use of pubsub for command execution allows for a nearly unbounded number of peers to contribute to a network queue. The leader receives this message and validates it before committing any mutations to the RAFT log. The log state is then synchronized to other electable peers via RAFT log replication, and published to all remaining peers over pubsub. As peers receive the update, they push it to the wrapper (again over ZMQ socket) which notifies the host process.","title":"Job operations"}]}